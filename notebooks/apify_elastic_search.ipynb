{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f138b410-f957-4323-a1f2-5bbb4adc1bf6",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to scrape potential job postings, and store them in Elasticsearch. The data will be subsequently analyzed in Kibana. We will use pre-written scrapers on [Apify's marketplace](https://console.apify.com/). Specifically, we will use [Google Jobs Scraper](https://console.apify.com/actors/SpK8RxKhIgV6BWOz9/console) actor. We will start by scraping the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8dd1cb0e-ae21-422f-ad14-82ebfce68ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "from apify_client import ApifyClient\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.client import IndicesClient\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0b211830-d261-435a-b7f2-3f1c95355ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENVIRONMENT VARIABLES\n",
    "APIFY_TOKEN = os.environ[\"apify_token\"]\n",
    "ELASTIC_PASSWORD = os.environ[\"ELASTIC_PASSWORD\"]\n",
    "\n",
    "# APIFY INPUT\n",
    "ACTOR_ID = \"SpK8RxKhIgV6BWOz9\"\n",
    "\n",
    "TITLES = [\n",
    "    \"Machine Learning Engineer\", \n",
    "    \"Data Scientist\", \n",
    "    \"MLOps Engineer\", \n",
    "    \"Data Analyst\", \n",
    "    \"Data Engineer\"\n",
    "]\n",
    "\n",
    "NUM_PAGES = 1\n",
    "MAX_CONCURRENCY = 10\n",
    "\n",
    "BASE_QUERIES = {\n",
    "    \"maxPagesPerQuery\": NUM_PAGES,\n",
    "    \"csvFriendlyOutput\": False,\n",
    "    \"countryCode\": \"ca\",\n",
    "    \"languageCode\": \"\",\n",
    "    \"maxConcurrency\": MAX_CONCURRENCY,\n",
    "    \"saveHtml\": False,\n",
    "    \"saveHtmlToKeyValueStore\": False,\n",
    "    \"includeUnfilteredResults\": False,\n",
    "}\n",
    "QUERY_URL = \"https://www.google.ca/search?q=JOB&ibp=htl;jobs&uule=w+CAIQICIGQ2FuYWRh\"\n",
    "\n",
    "# ELASTIC INPUT\n",
    "ELASTIC_HOST = 'http://localhost:9200'\n",
    "INDEX_NAME = 'jobs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1fa333-c137-42da-9f34-95790aa81f5d",
   "metadata": {},
   "source": [
    "Now, we prepare the queries based on the variables defined above and run the actors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2958087a-747e-4cf3-9697-321ca40294a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS QUERIES TO CREATE A QUERY FOR EACH SEARCHED TITLE\n",
    "processed_titles = [\"%20\".join(title.split()) for title in TITLES]\n",
    "query_urls = [re.sub(\"JOB\", title, QUERY_URL) for title in processed_titles]\n",
    "\n",
    "# PREPARE QUERIES\n",
    "queries = []\n",
    "for query_url in query_urls:\n",
    "    query = BASE_QUERIES.copy()\n",
    "    query[\"queries\"] = query_url\n",
    "    queries.append(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e88b6353-134e-46c1-9467-c8788af3d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ApifyClient with your API token\n",
    "client = ApifyClient(APIFY_TOKEN)\n",
    "\n",
    "items = []\n",
    "# Run the Actor and wait for it to finish\n",
    "for query in queries:\n",
    "    run = client.actor(ACTOR_ID).call(run_input=query)\n",
    "    \n",
    "    # Fetch and print Actor results from the run's dataset (if there are any)\n",
    "    for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "        items.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3948af72-0877-4484-83eb-f0118a303562",
   "metadata": {},
   "source": [
    "Finally, we create an index in Elasticsearch and add all the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2d5859dc-faa1-4cbe-8ac2-7237539af332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Elastic Search Client and Add the Scraped Records to the Index\n",
    "es = Elasticsearch(ELASTIC_HOST, basic_auth=(\"elastic\", ELASTIC_PASSWORD))\n",
    "\n",
    "# CREATE an Index to Store the Jobs\n",
    "if not IndicesClient(es).exists(index=INDEX_NAME):\n",
    "    es.indices.create(index=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fed86a64-8319-4080-a9a3-12da336361f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_id = 1\n",
    "for item in items:\n",
    "    for _, job_data in enumerate(item['googleJobs']):\n",
    "        es.index(\n",
    "            index=INDEX_NAME,\n",
    "            id=start_id,\n",
    "            document=job_data,\n",
    "        )\n",
    "        start_id += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
